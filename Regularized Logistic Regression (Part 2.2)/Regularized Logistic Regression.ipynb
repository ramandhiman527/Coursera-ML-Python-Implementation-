{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "588bc000",
   "metadata": {},
   "source": [
    "**Regularization is a process of introducing additional information in order to solve an ill-posed problem or to prevent overfitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c4dcb1",
   "metadata": {},
   "source": [
    "Similar to the linear regression, even logistic regression is prone to overfitting if there are large number of features. If the decision boundary is overfit, the shape might be highly contorted to fit only the training data while failing to generalise for the unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf55329",
   "metadata": {},
   "source": [
    "So, the cost function of the logistic regression is updated to penalize high values of the parameters and is given by,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6eb0ba",
   "metadata": {},
   "source": [
    "$ J(θ) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left(y^{(i)} log(h_θ(x^{(i)}) + (1 − y^{(i)}) log(1 − h_θ(x^{(i)}))\\right) + \\frac{λ}{2mn} \\sum_{j=1}^{n} θ^2_j $\n",
    "                                          \n",
    "+ Where\n",
    "                                          \n",
    " $ \\circ \\frac{λ}{2mn} \\sum_{j=1}^{n}θ^2_j $ is the regularization term.\n",
    "                                          \n",
    " $ \\circ λ $ is the regularization factor.                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50462427",
   "metadata": {},
   "source": [
    "### Regularization for Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d87bd90",
   "metadata": {},
   "source": [
    "The gradient descent for logistic regression without regularization was given by,\n",
    "    \n",
    "$ Repeat Until Convergence $\n",
    "\n",
    "$ \\{ \n",
    "     θ_j:= θ_j − α \\frac{1}{m} \\sum_{i=1}^{m} (h(x^{(i)})−y^{(i)}) x^{(i)}_j \n",
    "\\} $\n",
    "\n",
    "$ \\bullet $ where j ∈ {0,1,⋯,n} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0033da03",
   "metadata": {},
   "source": [
    "But since the equation for cost function has changed  to include the regularization term, there will be a **change in the derivative of cost function** that was plugged in the gradient descent algorithm,\n",
    "\n",
    "$ \\frac{∂}{∂θ_j} J(θ) = \\frac{∂}{∂θ_j} \\begin{bmatrix}−\\frac{1}{m}\\sum_{i=0}^{m} \\left(y^{(i)}log(h_θ(x^{(i)}) + (1−y^{(i)})log(1−h_θ(x^{(i)}))\\right) + \\frac{λ}{2mn} \\sum_{j=1}^{n} θ^2_j \\end{bmatrix} $\n",
    "                                                                                       \n",
    "                                                                                       \n",
    " $ = \\frac{1}{m} \\sum_{i=0}^{m} (h_θ(x^{(i)}) − y^{(i)}) x^{(i)}j + \\frac{λ}{m}θ_j $ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d14623a",
   "metadata": {},
   "source": [
    "Because the first term of cost fuction remains the same, so does the first term of the derivative. So taking derivative of second term gives  $ \\frac{λ}{m}θ_j $ as seen above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9408e7",
   "metadata": {},
   "source": [
    "updated as:\n",
    "    \n",
    "    \n",
    "Repeat until Convergence =  $ θ_0:= θ_0 - α\\begin{bmatrix}\\frac{1}{m}\\sum_{i=1}^{m}(h_θ(x^{(i)})−y^{(i)})x^{(i)}_0\\end{bmatrix} \\\\\n",
    "                              θ_j:= θ_j - α\\begin{bmatrix}\\frac{1}{m}\\sum_{i=1}^{m}(h_θ(x^{(i)})−y^{(i)})x^{(i)}_j + \\frac{λ}{m}\\theta_j \\end{bmatrix} $ \n",
    "\n",
    "+ Where j ∈ {1,2,⋯,n}and h is the **sigmoid function**.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ecae45",
   "metadata": {},
   "source": [
    "It can be noticed that, for case j=0, there is no regularization term included which is consistent with the convention followed for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae68591d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "mul = np.matmul\n",
    "\n",
    "\"\"\"\n",
    "X is the design matrix\n",
    "y is the target vector\n",
    "theta is the parameter vector\n",
    "lamda is the regularization parameter\n",
    "\"\"\"\n",
    "def sigmoid(X):\n",
    "    return np.power(1 + np.exp(-X), -1)\n",
    "\"\"\"\n",
    "hypothesis function\n",
    "\"\"\"\n",
    "def h(X, theta):\n",
    "    return sigmoid(mul(X, theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdad06ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "regularized cost function\n",
    "\"\"\"\n",
    "def j(theta, X, y, lamda=None):\n",
    "    m = X.shape[0]\n",
    "    theta[0] = 0\n",
    "    if lamda:\n",
    "        return (-(1/m) * (mul(y.T, np.log(h(X, theta))) + \\\n",
    "                          mul((1-y).T, np.log(1 - h(X, theta)))) + \\\n",
    "                (lamda/(2*m))*mul(theta.T, theta))[0][0] \n",
    "    return -(1/m) * (mul(y.T, np.log(h(X, theta))) + \\\n",
    "                     mul((1-y).T, np.log(1 - h(X, theta))))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cd87141",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "regularized cost gradient\n",
    "\"\"\"\n",
    "def j_prime(theta, X, y, lamda=None):\n",
    "    m = X.shape[0]\n",
    "    theta[0] = 0\n",
    "    if lamda:\n",
    "        return (1/m) * mul(X.T, (h(X, theta) - y)) + (lamda/m) * theta \n",
    "    return (1/m) * mul(X.T, (h(X, theta) - y)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d717f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simultaneous update\n",
    "\"\"\"\n",
    "def update_theta(theta, X, y, lamda=None):\n",
    "    return theta - alpha * j_prime(theta, X, y, lamda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e43458e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
